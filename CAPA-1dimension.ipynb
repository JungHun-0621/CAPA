{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c917e3f-a708-42d8-9edb-984b38619c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from scipy.io import savemat\n",
    "import random\n",
    "\n",
    "def h_broadcast(t, r, wavelength=0.01, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    eta = 120 * torch.pi\n",
    "    k0 = 2 * torch.pi / wavelength\n",
    "\n",
    "    # polarization vector e (y-direction)\n",
    "    e = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32, device=device).view(1, 3, 1)\n",
    "\n",
    "    # t: (100, 3), r: (50, 3)\n",
    "    # 확장해서 (50, 100, 3)\n",
    "    t_exp = t[None, :, :].expand(r.shape[0], -1, -1)   # (50, 100, 3)\n",
    "    r_exp = r[:, None, :].expand(-1, t.shape[0], -1)   # (50, 100, 3)\n",
    "\n",
    "    diff = r_exp - t_exp                      # (50, 100, 3)\n",
    "    norm = torch.norm(diff, dim=2, keepdim=True)       # (50, 100, 1)\n",
    "    norm_squeezed = norm.squeeze(-1)          # (50, 100)\n",
    "\n",
    "    phase_real = torch.cos(-k0 * norm_squeezed)  # (50, 100)\n",
    "    phase_imag = torch.sin(-k0 * norm_squeezed)  # (50, 100)\n",
    "    phase = torch.complex(phase_real, phase_imag)  # (50, 100)\n",
    "\n",
    "    # Projection matrix\n",
    "    diff_u = diff.unsqueeze(3)  # (50, 100, 3, 1)\n",
    "    outer = diff_u @ diff_u.transpose(2, 3) / (norm ** 2).unsqueeze(3)  # (50, 100, 3, 3)\n",
    "    I = torch.eye(3, device=device).view(1, 1, 3, 3).expand(r.shape[0], t.shape[0], 3, 3)\n",
    "    proj = I - outer  # (50, 100, 3, 3)\n",
    "\n",
    "    # Polarization response\n",
    "    e_exp = e.view(1, 1, 3, 1).expand(r.shape[0], t.shape[0], 3, 1)\n",
    "    polarization = (e_exp.transpose(2, 3) @ (proj @ e_exp)).squeeze(-1).squeeze(-1)  # (50, 100)\n",
    "\n",
    "    coeff = -1j * eta / (2 * wavelength * norm_squeezed)  # (50, 100)\n",
    "    return (coeff * phase * polarization)  # shape: (50, 100)\n",
    "\n",
    "\n",
    "loss_history = {\"LR1\": [], \"LR2\": [], \"LR3\": [], \"LR4\": [], \"LR5\": [], \"L_ic_op\": [], \"L_ic1\": [], \"L_ic2\": []}\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=nn.SiLU()):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.fc2 = nn.Linear(out_features, out_features)\n",
    "        if in_features != out_features:\n",
    "            self.skip = nn.Linear(in_features, out_features)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip(x)\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.activation(self.fc2(out))\n",
    "        out = out + residual\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e527e8-d6b5-4a19-b383-d3c1961ea2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complex_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_blocks, act=nn.SiLU(), out_act=None):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(hidden_dim, hidden_dim, act) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act = act\n",
    "        self.out_act = out_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.input_layer(x))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        # 출력 차원 처리\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 홀수이면 그대로 반환\n",
    "        if x.shape[1] % 2 != 0:\n",
    "            return x\n",
    "\n",
    "        # 짝수이면 복소수로 변환\n",
    "        half_dim = x.shape[1] // 2  # 절반 크기\n",
    "        x = x.view(batch_size, half_dim, 2)\n",
    "\n",
    "        # 복소수 변환\n",
    "        real_part = x[:, :, 0].unsqueeze(2)  # 실수부\n",
    "        imag_part = x[:, :, 1].unsqueeze(2)  # 허수부\n",
    "        complex_output = torch.complex(real_part, imag_part)\n",
    "\n",
    "        return complex_output\n",
    "\n",
    "class Real_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_blocks, act=nn.SiLU(), out_act=None):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(hidden_dim, hidden_dim, act) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act = act\n",
    "        self.out_act = out_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.input_layer(x))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db6bc9-c2f4-4bdb-9957-51ebe7c34ea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m user2 \u001b[38;5;241m=\u001b[39m pair[:, \u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     76\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m30.0\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m user1_3d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m user2_3d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([user2, z], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Channel\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "Signal = Complex_Model(input_dim=9, output_dim=4, hidden_dim=256, num_blocks=2)\n",
    "mp_integral = Complex_Model(input_dim=9, output_dim=8, hidden_dim=256, num_blocks=2)\n",
    "objective_integral = Real_Model(input_dim=9, output_dim=1, hidden_dim=256, num_blocks=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Signal.to(device)\n",
    "mp_integral.to(device)\n",
    "objective_integral.to(device)\n",
    "\n",
    "optimizer1 = optim.Adam(Signal.parameters(), lr=1e-3)\n",
    "optimizer2 = optim.Adam(mp_integral.parameters(), lr=1e-3)\n",
    "optimizer3 = optim.Adam(objective_integral.parameters(), lr=1e-3)\n",
    "\n",
    "N = 10\n",
    "\n",
    "# SINR Constraint\n",
    "gamma_db_1 = 5  # dB\n",
    "gamma_db_2 = 5  # dB\n",
    "gamma_p_1 = 10**(gamma_db_1 / 10)\n",
    "gamma_p_2 = 10**(gamma_db_2 / 10)\n",
    "\n",
    "sigma_sq = 1\n",
    "\n",
    "# Number of Users\n",
    "P = 2\n",
    "\n",
    "gamma_list = torch.tensor([gamma_p_1, gamma_p_2]) \n",
    "\n",
    "C_list = []\n",
    "\n",
    "for p in range(P):\n",
    "    c = torch.ones(P) / sigma_sq\n",
    "    c[p] = -1.0 / (gamma_list[p] * sigma_sq)\n",
    "    C_p = torch.diag(c).to(device)\n",
    "    C_list.append(C_p)\n",
    "    \n",
    "# C matrix for Constraint\n",
    "C_1, C_2 = C_list\n",
    "\n",
    "iterations = 100000\n",
    "num_tx = 500\n",
    "batch_size = 50\n",
    "Total = []\n",
    "\n",
    "for epoch in range(iterations):\n",
    "\n",
    "    # 손실항 초기화\n",
    "    L_ic1 = L_ic2 = L_ic_op = LR1 = LR2 = LR3 = LR4 = LR5 = 0.0\n",
    "    \n",
    "    # Gradient-Zero Condition\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()    \n",
    "    optimizer3.zero_grad()\n",
    "    \n",
    "    # 안테나 생성\n",
    "    tx = torch.tensor(np.random.uniform(-10, 10, num_tx), dtype=torch.float32, device=device).unsqueeze(1)   # (500, 1)\n",
    "    ty = torch.zeros((num_tx, 1), dtype=torch.float32, device=device)                                        # (500, 1)\n",
    "    tz = torch.zeros((num_tx, 1), dtype=torch.float32, device=device)                                        # (500, 1)\n",
    "    t = torch.cat([tx, ty, tz], dim=1)  # shape: (500, 3)\n",
    "    \n",
    "    # 유저1 생성\n",
    "    u1n = np.zeros((batch_size, 3))\n",
    "    u1n[:, 0] = np.random.uniform(-1, 1, batch_size)\n",
    "    u1n[:, 2] = 30\n",
    "    u1 = torch.from_numpy(u1n).float().to(device)\n",
    "\n",
    "    # 유저2 생성\n",
    "    u2n = np.zeros((batch_size, 3))\n",
    "    u2n[:, 0] = np.random.uniform(-1, 1, batch_size)\n",
    "    u2n[:, 2] = 30\n",
    "    u2 = torch.from_numpy(u2n).float().to(device)\n",
    "\n",
    "    u1_exp = u1[:, None, :].expand(-1, num_tx, -1)  \n",
    "    u2_exp = u2[:, None, :].expand(-1, num_tx, -1)  \n",
    "\n",
    "    t_exp = t[None, :, :].expand(batch_size, -1, -1)     \n",
    "\n",
    "    inputs = torch.cat([u1_exp, u2_exp, t_exp], dim=-1)\n",
    "\n",
    "    H1 = h_broadcast(t, u1)  # H1 shape: (50, 500)\n",
    "    H2 = h_broadcast(t, u2)\n",
    "\n",
    "    Signal_output = Signal(inputs.view(-1, 9)).view(batch_size, num_tx, 2)\n",
    "    mp_integral_output = mp_integral(inputs.view(-1, 9)).view(batch_size, num_tx, 2, 2)\n",
    "    objective_integral_output = objective_integral(inputs.view(-1, 9)).view(batch_size, num_tx, 1)\n",
    "\n",
    "    q1, q2 = Signal[:, :, 0], Signal_output[:, :, 1]\n",
    "\n",
    "\n",
    "    LR2 = F.mse_loss((torch.conj(h1) * q1).real, d2_mp[:, :, 0].real) + F.mse_loss((torch.conj(h1) * q1).imag, d2_mp[:, :, 0].imag)\n",
    "    LR3 = F.mse_loss((torch.conj(h1) * q2).real, d2_mp[:, :, 1].real) + F.mse_loss((torch.conj(h1) * q2).imag, d2_mp[:, :, 1].imag)\n",
    "    LR4 = F.mse_loss((torch.conj(h2) * q1).real, d2_mp[:, :, 2].real) + F.mse_loss((torch.conj(h2) * q1).imag, d2_mp[:, :, 2].imag)\n",
    "    LR5 = F.mse_loss((torch.conj(h2) * q2).real, d2_mp[:, :, 3].real) + F.mse_loss((torch.conj(h2) * q2).imag, d2_mp[:, :, 3].imag)\n",
    "\n",
    "\n",
    "\n",
    "    # 적분 항들\n",
    "    t_pos = torch.tensor([[10.0, 0.0, 30.0]], device=device).expand(batch_size, -1)\n",
    "    t_neg = torch.tensor([[-10.0, 0.0, 30.0]], device=device).expand(batch_size, -1)\n",
    "\n",
    "    input_pos = torch.cat([u1, u2, t_pos], dim=1)  # (B, 9)\n",
    "    input_neg = torch.cat([u1, u2, t_neg], dim=1)\n",
    "\n",
    "    val_pos = mp_integral(input_pos).view(batch_size,2,2)\n",
    "    val_neg = mp_integral(input_neg).view(batch_size,2,2)\n",
    "\n",
    "    int_mp = val_pos - val_neg\n",
    "\n",
    "    C_1 = C_1.to(dtype=torch.complex64, device=device)\n",
    "    C_2 = C_2.to(dtype=torch.complex64, device=device)\n",
    "\n",
    "    term1 = torch.einsum('bi,ij,bj->b', int_mp[:, 0].conj(), C_1, int_mp[:, 0])\n",
    "    term2 = torch.einsum('bi,ij,bj->b', int_mp[:, 1].conj(), C_2, int_mp[:, 1])\n",
    "\n",
    "    # 부등호 제약 조건\n",
    "    L_ic1 = torch.relu(term1.real + 1).sum()\n",
    "    L_ic2 = torch.relu(term2.real + 1).sum()\n",
    "\n",
    "\n",
    "    # 목적 함수\n",
    "    pos_obj = objective_integral(input_pos)\n",
    "    neg_obj = objective_integral(input_neg)\n",
    "    \n",
    "    int_obj = pos_obj - neg_obj\n",
    "    L_ic_op = int_obj.sum()\n",
    "\n",
    "\n",
    "\n",
    "    for key, value in zip(loss_history.keys(), [LR1, LR2, LR3, LR4, LR5, L_ic_op, L_ic1, L_ic2]):\n",
    "        loss_history[key].append(value.item())\n",
    "\n",
    "    Total_Loss = LR1 + 3*LR2 + 3*LR3 + 3*LR4 + 3*LR5 + L_ic1 + L_ic2 + 1e-8 * L_ic_op\n",
    "\n",
    "    Total_Loss.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "\n",
    "    Total.append(Total_Loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(epoch, f\"Total Loss: {Total_Loss.item():.6f}\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(Total, label=\"Total Loss\", color=np.random.rand(3,))\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Log Loss Value\")\n",
    "        plt.title(\"Loss during Training\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        for key, values in loss_history.items():\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            if key != \"L_ic_op\":\n",
    "                values = np.log10(np.maximum(np.abs(values), 1e-16))\n",
    "            plt.plot(values, label=key, color=np.random.rand(3,))\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Log Loss Value\" if key != \"L_ic_op\" else \"Loss Value\")\n",
    "            plt.title(f\"{key} During Training\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "for key, values in loss_history.items():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    if key != \"L_ic_op\":\n",
    "        values = np.log10(np.maximum(np.abs(values), 1e-16))\n",
    "    plt.plot(values, label=key, color=np.random.rand(3,))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Log Loss Value\" if key != \"L_ic_op\" else \"Loss Value\")\n",
    "    plt.title(f\"{key} During Training\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{key.replace(' ', '_')}_history.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3256a-eef1-4fec-a975-826fef34558b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
