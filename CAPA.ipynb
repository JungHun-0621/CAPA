{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c917e3f-a708-42d8-9edb-984b38619c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from scipy.io import savemat\n",
    "import random\n",
    "\n",
    "def h_single(t, r, wavelength=0.01, device='cpu'):\n",
    "    eta = 120 * torch.pi  # Free space impedance\n",
    "    k0 = 2 * torch.pi / wavelength\n",
    "\n",
    "    # polarization vector e (y-direction)\n",
    "    e = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32, device=device).view(1, 3, 1)\n",
    "\n",
    "    # Distance vector and norm\n",
    "    diff = r - t                             # (N, 3)\n",
    "    norm = torch.norm(diff, dim=1, keepdim=True)  # (N, 1)\n",
    "    norm_squeezed = norm.squeeze(1)  # (N,)\n",
    "\n",
    "    # Phase term: e^{-j k0 r} = cos(-k0 r) + j sin(-k0 r)\n",
    "    phase_real = torch.cos(-k0 * norm_squeezed)  # (N,)\n",
    "    phase_imag = torch.sin(-k0 * norm_squeezed)  # (N,)\n",
    "    phase = torch.complex(phase_real, phase_imag)  # (N,)\n",
    "\n",
    "    # Projection matrix\n",
    "    diff_u = diff.unsqueeze(2)                            # (N, 3, 1)\n",
    "    norm_sq = norm.pow(2).unsqueeze(2)                    # (N, 1, 1)\n",
    "    outer = torch.bmm(diff_u, diff_u.transpose(1, 2)) / norm_sq  # (N, 3, 3)\n",
    "    I = torch.eye(3, device=device).expand(t.shape[0], 3, 3)         # (N, 3, 3)\n",
    "    proj = I - outer                                       # (N, 3, 3)\n",
    "\n",
    "    # Polarization response\n",
    "    e_expanded = e.expand(t.shape[0], -1, -1)              # (N, 3, 1)\n",
    "    polarization = torch.bmm(e_expanded.transpose(1, 2), torch.bmm(proj, e_expanded)).squeeze()  # (N,)\n",
    "\n",
    "    # Final coefficient\n",
    "    coeff = -1j * eta / (2 * wavelength * norm_squeezed)  # (N,)\n",
    "    \n",
    "    return (coeff * phase * polarization).unsqueeze(1)\n",
    "\n",
    "# 정적분 변환\n",
    "def evaluate_integral(model, user_1, user_2, device, is_complex=True):\n",
    "    N = 10\n",
    "    tx_vals = [-N, N]\n",
    "    ty_vals = [-N, N]\n",
    "    inputs = []\n",
    "\n",
    "    for tx in tx_vals:\n",
    "        for ty in ty_vals:\n",
    "            t = torch.tensor([[tx, ty]], dtype=torch.float32).to(device)\n",
    "            J_input = torch.cat([user_1, user_2], dim=1).to(device)\n",
    "            full_input = torch.cat([t, J_input], dim=1)\n",
    "            inputs.append(full_input)\n",
    "\n",
    "    inputs = torch.cat(inputs, dim=0)  # shape: (4, input_dim)\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    if is_complex:\n",
    "        result = outputs[3] - outputs[1] - outputs[2] + outputs[0]\n",
    "    else:\n",
    "        result = outputs[3] - outputs[1] - outputs[2] + outputs[0]\n",
    "        result = result.squeeze()  # (1,)\n",
    "\n",
    "    return result\n",
    "\n",
    "# 이중 미분 함수\n",
    "def compute_second_order_derivative(output, t_x, t_y):\n",
    "    if torch.is_complex(output):\n",
    "        results = []\n",
    "        for i in range(output.shape[1]):  # for each output dimension\n",
    "            grad_tx_real = torch.autograd.grad(output[:, i].real.sum(), t_x, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "            grad_ty_real = torch.autograd.grad(grad_tx_real.sum(), t_y, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "\n",
    "            grad_tx_imag = torch.autograd.grad(output[:, i].imag.sum(), t_x, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "            grad_ty_imag = torch.autograd.grad(grad_tx_imag.sum(), t_y, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "\n",
    "            d2 = torch.complex(grad_ty_real, grad_ty_imag)  # shape: (1000,)\n",
    "            results.append(d2.unsqueeze(1))  # shape: (1000,1)\n",
    "\n",
    "        return torch.cat(results, dim=1)  # (1000, 4)\n",
    "\n",
    "    else:\n",
    "        results = []\n",
    "        for i in range(output.shape[1]):\n",
    "            grad_tx = torch.autograd.grad(output[:, i].sum(), t_x, create_graph=True, retain_graph=True)[0]\n",
    "            grad_ty = torch.autograd.grad(grad_tx.sum(), t_y, create_graph=True)[0]\n",
    "            results.append(grad_ty.unsqueeze(1))\n",
    "        return torch.cat(results, dim=1)  # (1000, 4)\n",
    "\n",
    "def generate_coordinates(num_points, x_range, y_range):\n",
    "    x = np.random.uniform(x_range[0], x_range[1], num_points)\n",
    "    y = np.random.uniform(y_range[0], y_range[1], num_points)\n",
    "    coordinates = np.stack((x, y), axis=1)  # (num_points, 2)\n",
    "    return torch.tensor(coordinates, dtype=torch.float32)\n",
    "\n",
    "def generate_user_pairs(batch_size):\n",
    "    user_pairs = generate_coordinates(batch_size * 2, x_range=(-1, 1), y_range=(-1, 1))\n",
    "    return user_pairs.view(batch_size, 2, 2)  # (batch_size, 2, 2)\n",
    "\n",
    "loss_history = {\"LR1\": [], \"LR2\": [], \"LR3\": [], \"LR4\": [], \"LR5\": [], \"L_ic_op\": [], \"L_ic1\": [], \"L_ic2\": []}\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=nn.SiLU()):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.fc2 = nn.Linear(out_features, out_features)\n",
    "        if in_features != out_features:\n",
    "            self.skip = nn.Linear(in_features, out_features)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip(x)\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.activation(self.fc2(out))\n",
    "        out = out + residual\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e527e8-d6b5-4a19-b383-d3c1961ea2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complex_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_blocks, act=nn.SiLU(), out_act=None):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(hidden_dim, hidden_dim, act) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act = act\n",
    "        self.out_act = out_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.input_layer(x))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        # 출력 차원 처리\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 홀수이면 그대로 반환\n",
    "        if x.shape[1] % 2 != 0:\n",
    "            return x\n",
    "\n",
    "        # 짝수이면 복소수로 변환\n",
    "        half_dim = x.shape[1] // 2  # 절반 크기\n",
    "        x = x.view(batch_size, half_dim, 2)\n",
    "\n",
    "        # 복소수 변환\n",
    "        real_part = x[:, :, 0].unsqueeze(2)  # 실수부\n",
    "        imag_part = x[:, :, 1].unsqueeze(2)  # 허수부\n",
    "        complex_output = torch.complex(real_part, imag_part)\n",
    "\n",
    "        return complex_output\n",
    "\n",
    "class Real_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_blocks, act=nn.SiLU(), out_act=None):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(hidden_dim, hidden_dim, act) for _ in range(num_blocks)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act = act\n",
    "        self.out_act = out_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.input_layer(x))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28db6bc9-c2f4-4bdb-9957-51ebe7c34ea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m user2 \u001b[38;5;241m=\u001b[39m pair[:, \u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     76\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m30.0\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m user1_3d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m user2_3d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([user2, z], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Channel\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "Signal = Complex_Model(input_dim=6, output_dim=4, hidden_dim=256, num_blocks=2)\n",
    "mp_integral = Complex_Model(input_dim=6, output_dim=8, hidden_dim=256, num_blocks=2)\n",
    "objective_integral = Real_Model(input_dim=6, output_dim=1, hidden_dim=256, num_blocks=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Signal.to(device)\n",
    "mp_integral.to(device)\n",
    "objective_integral.to(device)\n",
    "\n",
    "optimizer1 = optim.Adam(Signal.parameters(), lr=1e-3)\n",
    "optimizer2 = optim.Adam(mp_integral.parameters(), lr=1e-3)\n",
    "optimizer3 = optim.Adam(objective_integral.parameters(), lr=1e-3)\n",
    "\n",
    "N = 10\n",
    "\n",
    "# SINR Constraint\n",
    "gamma_db_1 = 5  # dB\n",
    "gamma_db_2 = 5  # dB\n",
    "gamma_p_1 = 10**(gamma_db_1 / 10)\n",
    "gamma_p_2 = 10**(gamma_db_2 / 10)\n",
    "\n",
    "sigma_sq = 1\n",
    "\n",
    "# Number of Users\n",
    "P = 2\n",
    "\n",
    "gamma_list = torch.tensor([gamma_p_1, gamma_p_2]) \n",
    "\n",
    "C_list = []\n",
    "\n",
    "for p in range(P):\n",
    "    c = torch.ones(P) / sigma_sq\n",
    "    c[p] = -1.0 / (gamma_list[p] * sigma_sq)\n",
    "    C_p = torch.diag(c).to(device)\n",
    "    C_list.append(C_p)\n",
    "    \n",
    "# C matrix for Constraint\n",
    "C_1, C_2 = C_list\n",
    "\n",
    "iterations = 100000\n",
    "num_tx = 100\n",
    "batch_size = 20\n",
    "Total = []\n",
    "\n",
    "for epoch in range(iterations):\n",
    "\n",
    "    # Gradient-Zero Condition\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()    \n",
    "    optimizer3.zero_grad()\n",
    "    \n",
    "    t_n_x = np.random.uniform(low=-10, high=10, size=(100, 1))\n",
    "    t_n_y = np.random.uniform(low=-10, high=10, size=(100, 1))\n",
    "\n",
    "    tx = torch.tensor(t_n_x, dtype=torch.float32, requires_grad=True).to(device)\n",
    "    ty = torch.tensor(t_n_y, dtype=torch.float32, requires_grad=True).to(device)\n",
    "    t = torch.cat([tx, ty], dim=1)\n",
    "    tz = torch.zeros_like(t[:, :1]).to(device).requires_grad_(True)\n",
    "    t_3d = torch.cat([t, tz], dim=1)\n",
    "\n",
    "    user_pairs = generate_user_pairs(batch_size).to(device).requires_grad_(True)\n",
    "\n",
    "    L_ic1 = L_ic2 = L_ic_op = LR1 = LR2 = LR3 = LR4 = LR5 = 0.0\n",
    "\n",
    "    for pair in user_pairs:\n",
    "        user10, user20 = pair.unbind(0)\n",
    "        user10 = user10.unsqueeze(0)\n",
    "        user20 = user20.unsqueeze(0)\n",
    "\n",
    "        pair = pair.unsqueeze(0).expand(num_tx, -1, -1)\n",
    "        user1 = pair[:, 0, :]\n",
    "        user2 = pair[:, 1, :]\n",
    "\n",
    "        z = torch.full((100, 1), 30.0)\n",
    "        user1_3d = torch.cat([user1, z], dim=1)\n",
    "        user2_3d = torch.cat([user2, z], dim=1)\n",
    "\n",
    "        # Channel\n",
    "        h1 = h_single(t_3d, user1_3d, wavelength=0.01, device=device)\n",
    "        h2 = h_single(t_3d, user2_3d, wavelength=0.01, device=device)\n",
    "\n",
    "        flattened_view = pair.view(100, -1).requires_grad_(True)\n",
    "        Input = torch.cat([t, flattened_view], dim=1)\n",
    "\n",
    "        Signal_output = Signal(Input).squeeze(-1)\n",
    "        mp_integral_output = mp_integral(Input).squeeze(-1)\n",
    "        objective_integral_output = objective_integral(Input).squeeze(-1)\n",
    "\n",
    "        # Signal\n",
    "        q1, q2 = Signal_output[:, 0], Signal_output[:, 1]\n",
    "\n",
    "        # 2-time Derivative\n",
    "        results = []\n",
    "        for i in range(mp_integral_output.shape[1]):\n",
    "            # 실수부 미분\n",
    "            grad_tx_real = torch.autograd.grad(mp_integral_output[:, i].real.sum(), tx, create_graph=True)[0]\n",
    "            grad_ty_real = torch.autograd.grad(grad_tx_real.sum(), ty, create_graph=True)[0]\n",
    "\n",
    "            # 허수부 미분\n",
    "            grad_tx_imag = torch.autograd.grad(mp_integral_output[:, i].imag.sum(), tx, create_graph=True)[0]\n",
    "            grad_ty_imag = torch.autograd.grad(grad_tx_imag.sum(), ty, create_graph=True)[0]\n",
    "\n",
    "            # 복소수 결합\n",
    "            d2 = torch.complex(grad_ty_real, grad_ty_imag)\n",
    "            results.append(d2.unsqueeze(1))\n",
    "\n",
    "        # 최종 결합\n",
    "        d2_mp = torch.cat(results, dim=1)\n",
    "\n",
    "        grad_tx = torch.autograd.grad(objective_integral_output.sum(), tx, create_graph=True)[0]\n",
    "        grad_ty = torch.autograd.grad(grad_tx.sum(), ty, create_graph=True)[0]\n",
    "        d2_obj = grad_ty\n",
    "\n",
    "        # integrate\n",
    "        int_mp = evaluate_integral(mp_integral, user10, user20, device, is_complex=True)\n",
    "        int_obj = evaluate_integral(objective_integral, user10, user20, device, is_complex=False)\n",
    "\n",
    "        M = int_mp.view(2, 2)\n",
    "        C_1 = C_1.to(dtype=torch.complex64, device=device)\n",
    "        C_2 = C_2.to(dtype=torch.complex64, device=device)\n",
    "        term1 = torch.conj(M[0]) @ C_1 @ M[0]\n",
    "        term2 = torch.conj(M[1]) @ C_2 @ M[1]\n",
    "\n",
    "        L_ic1 += torch.relu(term1.real + 1)\n",
    "        L_ic2 += torch.relu(term2.real + 1)\n",
    "        L_ic_op += int_obj\n",
    "\n",
    "        LR1 += mse(q1.real**2 + q1.imag**2 + q2.real**2 + q2.imag**2, d2_obj.view(-1))\n",
    "        LR2 += mse((torch.conj(h1) * q1.unsqueeze(1)).real, d2_mp[:,0].real) + mse((torch.conj(h1) * q1.unsqueeze(1)).imag, d2_mp[:,0].imag)\n",
    "        LR3 += mse((torch.conj(h1) * q2.unsqueeze(1)).real, d2_mp[:,1].real) + mse((torch.conj(h1) * q2.unsqueeze(1)).imag, d2_mp[:,1].imag)\n",
    "        LR4 += mse((torch.conj(h2) * q1.unsqueeze(1)).real, d2_mp[:,2].real) + mse((torch.conj(h2) * q1.unsqueeze(1)).imag, d2_mp[:,2].imag)\n",
    "        LR5 += mse((torch.conj(h2) * q2.unsqueeze(1)).real, d2_mp[:,3].real) + mse((torch.conj(h2) * q2.unsqueeze(1)).imag, d2_mp[:,3].imag)\n",
    "\n",
    "    for key, value in zip(loss_history.keys(), [LR1, LR2, LR3, LR4, LR5, L_ic_op, L_ic1, L_ic2]):\n",
    "        loss_history[key].append(value.item())\n",
    "\n",
    "    Total_Loss = LR1 + 3*LR2 + 3*LR3 + 3*LR4 + 3*LR5 + L_ic1 + L_ic2 + 1e-8 * L_ic_op\n",
    "\n",
    "    Total_Loss.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "\n",
    "    Total.append(Total_Loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(epoch, f\"Total Loss: {Total_Loss.item():.6f}\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(Total, label=\"Total Loss\", color=np.random.rand(3,))\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Log Loss Value\")\n",
    "        plt.title(\"Loss during Training\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        for key, values in loss_history.items():\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            if key != \"L_ic_op\":\n",
    "                values = np.log10(np.maximum(np.abs(values), 1e-16))\n",
    "            plt.plot(values, label=key, color=np.random.rand(3,))\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Log Loss Value\" if key != \"L_ic_op\" else \"Loss Value\")\n",
    "            plt.title(f\"{key} During Training\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "for key, values in loss_history.items():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    if key != \"L_ic_op\":\n",
    "        values = np.log10(np.maximum(np.abs(values), 1e-16))\n",
    "    plt.plot(values, label=key, color=np.random.rand(3,))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Log Loss Value\" if key != \"L_ic_op\" else \"Loss Value\")\n",
    "    plt.title(f\"{key} During Training\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{key.replace(' ', '_')}_history.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3256a-eef1-4fec-a975-826fef34558b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
